{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOClWqNsTIpANUM6Vyu4w9u"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8O2qLxi_Gmz_","outputId":"477fdc2c-436e-4040-ce98-7ba2f5527d9e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Chatbot: Hi! How can I help you today?\n","You: hi! how do you do. my name is Doi. may i have your name?\n","Chatbot: I'm not sure what you mean by that.\n","You: i just want to know your name\n","Chatbot: i want to be your friend\n","You: yes, you are my frined\n","Chatbot: i am your friend!\n","You: yes, of course\n","Chatbot: yes, you are frined!\n","You: so i want to know my friend' name.\n","Chatbot: i'm do\n","You: what is your name?\n","Chatbot: \n","You: you do not like me?\n","Chatbot:  i\n","You: you are my best friend.\n","Chatbot: \n","You: do you like me\n","Chatbot: \n","You: you want to kiss me\n","Chatbot:  i love\n","You: and may i have your name\n","Chatbot: \n","You: when can i get your name?\n","Chatbot:  me\n","You: what do you think about global warming\n","Chatbot: \n","You: \n","Chatbot: \n","You: \n","Chatbot:  i?\n","You: global warming\n","Chatbot:  i i\n","You: \n","Chatbot: ?\n","You: cat\n","Chatbot: \n","You: please help me\n","Chatbot:  u\n","You: why don't you help me?\n","Chatbot:  u?\n","You: yes me\n","Chatbot: ? i\n","You: please help me\n","Chatbot:  me?\n","You: do you know mt fuji?\n","Chatbot: ? u\n","You: mountain in Japan\n","Chatbot: \n","You: \n","Chatbot:  u i\n","You: can you go to mt Fuji with me today?\n","Chatbot:  you\n","You: yes you\n","Chatbot: \n","You: \n","Chatbot:  you?\n","You: no,me. You\n","Chatbot: \n","You: \n","Chatbot:  I\n","You: please help me\n","Chatbot:  I i\n","You: yes, you\n","Chatbot:  i u\n","You: what is 4990-5220\n","Chatbot: \n","You: can you calculate 4990 - 5220\n","Chatbot:  i.\n","You: yes, you\n","Chatbot:  u.\n","You: please help me\n","Chatbot: .\n","You: you are not good at calculation,right\n","Chatbot: \n","You: please help me\n","Chatbot:  to\n","You: to do calculation 4990 - 5220\n","Chatbot:  u u\n","You: may i have your name?\n","Chatbot: . u\n","You: can i get your name?\n","Chatbot:  I?\n","You: yes, your name\n","Chatbot:  u c\n"]}],"source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","# モデル名の設定\n","model_name = \"microsoft/DialoGPT-medium\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForCausalLM.from_pretrained(model_name)\n","\n","# モデルをGPUに移動\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# チャットボットとして使用\n","def chat_with_bot(model, tokenizer):\n","    print(\"Chatbot: Hi! How can I help you today?\")\n","    chat_history_ids = None\n","\n","    while True:\n","        user_input = input(\"You: \")\n","        if user_input.lower() in ['exit', 'quit', 'bye']:\n","            print(\"Chatbot: Goodbye!\")\n","            break\n","\n","        new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt').to(device)\n","\n","        # Combine chat history and new user input\n","        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if chat_history_ids is not None else new_user_input_ids\n","\n","        # Generate response with a limit of 40 new tokens\n","        chat_history_ids = model.generate(\n","            bot_input_ids,\n","            max_new_tokens=40,\n","            pad_token_id=tokenizer.eos_token_id,\n","            no_repeat_ngram_size=3,\n","        )\n","\n","        response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n","        print(f\"Chatbot: {response}\")\n","\n","        # Update chat history\n","        chat_history_ids = chat_history_ids if chat_history_ids is not None else new_user_input_ids\n","        chat_history_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1)\n","\n","# チャットを開始\n","chat_with_bot(model, tokenizer)\n"]}]}